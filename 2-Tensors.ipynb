{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch 张量\n",
    "\n",
    "张量（Tensors）是 PyTorch 中的核心数据抽象。这个交互式笔记本提供了对 `torch.Tensor` 类的深入介绍。您可以在本地、[Colab](https://colab.research.google.com/) 或您喜欢的云服务上运行此笔记本。\n",
    "\n",
    "首先，让我们导入 PyTorch 模块。我们还将添加 Python 的 math 模块以便于后续示例的演示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入 PyTorch 和 math 模块\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建张量\n",
    "创建张量最简单的方法是使用 `torch.empty()` 函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个 3x4 的空张量\n",
    "x = torch.empty(3, 4)\n",
    "print(type(x))  # 打印张量的类型\n",
    "print(x)        # 打印张量的值 (未初始化)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们来解读一下刚才的操作：\n",
    "\n",
    "* 我们使用 `torch` 模块中的众多工厂方法之一创建了一个张量。\n",
    "* 这个张量本身是二维的，具有 3 行和 4 列。\n",
    "* 返回的对象类型是 `torch.Tensor`，它是 `torch.FloatTensor` 的别名；默认情况下，PyTorch 张量使用 32 位浮点数填充。（关于数据类型的更多内容将在后面介绍。）\n",
    "* 打印张量时，您可能会看到一些看似随机的值。`torch.empty()` 函数分配了张量的内存，但没有用任何值对其进行初始化——因此您看到的是分配时内存中的内容。\n",
    "\n",
    "关于张量及其维度数量和术语的简要说明：\n",
    "* 您有时会看到一维张量被称为 *向量*。\n",
    "* 同样，二维张量通常被称为 *矩阵*。\n",
    "* 任何超过二维的张量通常都被称为张量。\n",
    "\n",
    "通常情况下，您会希望用某些值初始化张量。常见的情况是全零、全一或随机值，`torch` 模块提供了所有这些的工厂方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n"
     ]
    }
   ],
   "source": [
    "# 创建全零张量\n",
    "zeros = torch.zeros(2, 3)\n",
    "print(zeros)\n",
    "\n",
    "# 创建全一张量\n",
    "ones = torch.ones(2, 3)\n",
    "print(ones)\n",
    "\n",
    "# 设置随机数种子并创建随机张量\n",
    "torch.manual_seed(1729) # 设置种子以确保可重复性\n",
    "random = torch.rand(2, 3)\n",
    "print(random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些工厂方法的功能正如您所期望的那样——我们有一个全零的张量，另一个全一的张量，以及一个值在 0 到 1 之间的随机值张量。\n",
    "\n",
    "### 随机张量和种子\n",
    "\n",
    "说到随机张量，您是否注意到在它之前调用了 `torch.manual_seed()`？用随机值初始化张量（例如模型的学习权重）是很常见的，但有时——尤其是在研究环境中——您希望确保结果的可重复性。手动设置随机数生成器的种子是实现这一目标的方法。让我们更仔细地看看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子并生成随机张量\n",
    "torch.manual_seed(1729)\n",
    "random1 = torch.rand(2, 3)\n",
    "print(random1)\n",
    "\n",
    "# 不重置种子，生成下一个随机张量\n",
    "random2 = torch.rand(2, 3)\n",
    "print(random2)\n",
    "\n",
    "# 再次设置相同的随机种子\n",
    "torch.manual_seed(1729)\n",
    "random3 = torch.rand(2, 3)\n",
    "print(random3) # random3 的值应与 random1 相同\n",
    "\n",
    "# 不重置种子，生成下一个随机张量\n",
    "random4 = torch.rand(2, 3)\n",
    "print(random4) # random4 的值应与 random2 相同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面应该可以看到 `random1` 和 `random3` 包含相同的值，`random2` 和 `random4` 也是如此。手动设置随机数生成器（RNG）的种子会重置它，因此在大多数情况下，依赖于随机数的相同计算应该产生相同的结果。\n",
    "\n",
    "欲了解更多信息，请参阅 [PyTorch 关于可重现性的文档](https://pytorch.org/docs/stable/notes/randomness.html)。\n",
    "\n",
    "### 张量形状\n",
    "通常，当您对两个或更多张量执行操作时，它们需要具有相同的*形状*——即具有相同的维度数量和每个维度中相同的单元格数量。为此，我们有 `torch.*_like()` 系列方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0.6128, 0.1519, 0.0453],\n",
      "         [0.5035, 0.9978, 0.3884]],\n",
      "\n",
      "        [[0.6929, 0.1703, 0.1384],\n",
      "         [0.4759, 0.7481, 0.0361]]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个三维张量\n",
    "x = torch.empty(2, 2, 3)\n",
    "print(x.shape)  # 打印张量的形状\n",
    "print(x)\n",
    "\n",
    "# 创建与 x 形状相同的空张量\n",
    "empty_like_x = torch.empty_like(x)\n",
    "print(empty_like_x.shape)\n",
    "print(empty_like_x)\n",
    "\n",
    "# 创建与 x 形状相同的全零张量\n",
    "zeros_like_x = torch.zeros_like(x)\n",
    "print(zeros_like_x.shape)\n",
    "print(zeros_like_x)\n",
    "\n",
    "# 创建与 x 形状相同的全一张量\n",
    "ones_like_x = torch.ones_like(x)\n",
    "print(ones_like_x.shape)\n",
    "print(ones_like_x)\n",
    "\n",
    "# 创建与 x 形状相同的随机张量\n",
    "rand_like_x = torch.rand_like(x)\n",
    "print(rand_like_x.shape)\n",
    "print(rand_like_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面代码单元中的第一个新内容是在张量上使用 `.shape` 属性。此属性包含张量每个维度大小的列表——在我们的例子中，`x` 是一个三维张量，形状为 2 x 2 x 3。\n",
    "\n",
    "在这之下，我们调用了 `.empty_like()`、`.zeros_like()`、`.ones_like()` 和 `.rand_like()` 方法。使用 `.shape` 属性，我们可以验证这些方法每一个都返回一个具有相同维度和大小的张量。\n",
    "\n",
    "创建张量的最后一种方法是直接从 Python 集合中指定其数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1416, 2.7183],\n",
      "        [1.6180, 0.0073]])\n",
      "tensor([ 2,  3,  5,  7, 11, 13, 17, 19])\n",
      "tensor([[2, 4, 6],\n",
      "        [3, 6, 9]])\n"
     ]
    }
   ],
   "source": [
    "# 从嵌套列表创建张量\n",
    "some_constants = torch.tensor([[3.1415926, 2.71828], [1.61803, 0.0072897]])\n",
    "print(some_constants)\n",
    "\n",
    "# 从元组创建张量\n",
    "some_integers = torch.tensor((2, 3, 5, 7, 11, 13, 17, 19))\n",
    "print(some_integers)\n",
    "\n",
    "# 从嵌套元组和列表创建张量\n",
    "more_integers = torch.tensor(((2, 4, 6), [3, 6, 9]))\n",
    "print(more_integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 `torch.tensor()` 是最直接的创建张量的方法，如果您已经有数据存储在 Python 的元组或列表中。如上所示，嵌套集合将生成一个多维张量。\n",
    "\n",
    "*注意：`torch.tensor()` 会创建数据的副本。*\n",
    "\n",
    "### 张量数据类型\n",
    "设置张量的数据类型有几种方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[ 0.9956,  1.4148,  5.8364],\n",
      "        [11.2406, 11.2083, 11.6692]], dtype=torch.float64)\n",
      "tensor([[ 0,  1,  5],\n",
      "        [11, 11, 11]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# 创建一个 int16 类型的张量\n",
    "a = torch.ones((2, 3), dtype=torch.int16) # 指定数据类型为 16 位整数\n",
    "print(a)\n",
    "\n",
    "# 创建一个 float64 类型的张量并乘以 20\n",
    "b = torch.rand((2, 3), dtype=torch.float64) * 20. # 指定数据类型为 64 位浮点数\n",
    "print(b)\n",
    "\n",
    "# 将张量 b 转换为 int32 类型\n",
    "c = b.to(torch.int32) # 使用 .to() 方法转换数据类型\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置张量的底层数据类型最简单的方法是在创建时使用可选参数。在上面代码单元的第一行中，我们为张量 `a` 设置了 `dtype=torch.int16`。当我们打印 `a` 时，可以看到它充满了 `1` 而不是 `1.`——Python 的微妙提示表明这是一个整数类型而不是浮点类型。\n",
    "\n",
    "打印 `a` 时的另一个需要注意的地方是，与我们将 `dtype` 保留为默认值（32 位浮点数）时不同，打印张量还会指定其 `dtype`。\n",
    "\n",
    "您可能还注意到，我们从将张量的形状指定为一系列整数参数，转变为将这些参数分组到一个元组中。这不是严格必要的——PyTorch 会将一系列初始的未标记整数参数作为张量形状——但在添加可选参数时，它可以使您的意图更易读。\n",
    "\n",
    "设置数据类型的另一种方法是使用 `.to()` 方法。在上面的代码单元中，我们以通常的方式创建了一个随机浮点张量 `b`。随后，我们通过使用 `.to()` 方法将 `b` 转换为 32 位整数来创建 `c`。请注意，`c` 包含与 `b` 相同的所有值，但被截断为整数。\n",
    "\n",
    "可用的数据类型包括：\n",
    "\n",
    "* `torch.bool`\n",
    "* `torch.int8`\n",
    "* `torch.uint8`\n",
    "* `torch.int16`\n",
    "* `torch.int32`\n",
    "* `torch.int64`\n",
    "* `torch.half`\n",
    "* `torch.float`\n",
    "* `torch.double`\n",
    "* `torch.bfloat`\n",
    "\n",
    "## 使用 PyTorch 张量进行数学和逻辑运算\n",
    "\n",
    "现在您已经了解了一些创建张量的方法……您可以用它们做什么呢？\n",
    "\n",
    "让我们先看看基本的算术运算，以及张量如何与简单的标量交互："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "tensor([[4., 4.],\n",
      "        [4., 4.]])\n",
      "tensor([[1.4142, 1.4142],\n",
      "        [1.4142, 1.4142]])\n"
     ]
    }
   ],
   "source": [
    "# 张量与标量的算术运算\n",
    "ones = torch.zeros(2, 2) + 1 # 全零张量加 1\n",
    "twos = torch.ones(2, 2) * 2  # 全一张量乘 2\n",
    "threes = (torch.ones(2, 2) * 7 - 1) / 2 # 组合运算\n",
    "fours = twos ** 2            # 平方运算\n",
    "sqrt2s = twos ** 0.5         # 开方运算\n",
    "\n",
    "print(ones)\n",
    "print(twos)\n",
    "print(threes)\n",
    "print(fours)\n",
    "print(sqrt2s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上所示，张量与标量之间的算术运算（如加法、减法、乘法、除法和指数运算）会分布到张量的每个元素上。由于此类运算的输出将是一个张量，您可以按照通常的运算符优先级规则将它们链接在一起，如我们创建 `threes` 的那一行。\n",
    "\n",
    "两个张量之间的类似运算也表现得像您直观上期望的那样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  4.],\n",
      "        [ 8., 16.]])\n",
      "tensor([[5., 5.],\n",
      "        [5., 5.]])\n",
      "tensor([[12., 12.],\n",
      "        [12., 12.]])\n"
     ]
    }
   ],
   "source": [
    "# 张量之间的算术运算 (按元素)\n",
    "powers2 = twos ** torch.tensor([[1, 2], [3, 4]]) # 按元素求幂\n",
    "print(powers2)\n",
    "\n",
    "fives = ones + fours # 按元素相加\n",
    "print(fives)\n",
    "\n",
    "dozens = threes * fours # 按元素相乘\n",
    "print(dozens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里需要注意的是，上一个代码单元中的所有张量都是相同形状的。如果我们尝试对形状不同的张量执行二元运算会发生什么？\n",
    "\n",
    "**注意：以下代码单元会抛出运行时错误。这是故意的。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试对形状不同的张量进行运算 (会导致错误)\n",
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(3, 2)\n",
    "\n",
    "# print(a * b) # 这行会报错，因为形状不匹配，无法进行按元素乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在一般情况下，您不能以这种方式对形状不同的张量进行运算，即使在上面的代码单元中，张量具有相同数量的元素。\n",
    "\n",
    "### 简述：张量广播\n",
    "\n",
    "*(注意：如果您熟悉 NumPy ndarray 的广播语义，您会发现这里的规则是相同的。)*\n",
    "\n",
    "同形状规则的例外是*张量广播*。以下是一个示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2024, 0.5731, 0.7191, 0.4067],\n",
      "        [0.7301, 0.6276, 0.7357, 0.0381]])\n",
      "tensor([[0.4049, 1.1461, 1.4382, 0.8134],\n",
      "        [1.4602, 1.2551, 1.4715, 0.0762]])\n"
     ]
    }
   ],
   "source": [
    "# 张量广播示例\n",
    "rand = torch.rand(2, 4)\n",
    "doubled = rand * (torch.ones(1, 4) * 2) # (2, 4) * (1, 4)\n",
    "# (1, 4) 张量会被广播到 (2, 4) 以匹配 rand 的形状\n",
    "\n",
    "print(rand)\n",
    "print(doubled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的技巧是什么？我们如何能够将一个 2x4 张量乘以一个 1x4 张量？\n",
    "\n",
    "广播是一种在形状相似的张量之间执行运算的方法。在上面的示例中，一个一行四列的张量被乘以一个两行四列张量的*每一行*。\n",
    "\n",
    "这是深度学习中的一个重要操作。常见的示例是将学习权重的张量乘以一个*批量*输入张量，对批量中的每个实例单独应用运算，并返回一个形状相同的张量——就像我们的 (2, 4) * (1, 4) 示例返回了一个形状为 (2, 4) 的张量。\n",
    "\n",
    "广播的规则是：\n",
    "\n",
    "* 每个张量必须至少有一个维度——没有空张量。\n",
    "* 比较两个张量的维度大小，*从最后一个维度到第一个维度：*\n",
    "* * 每个维度必须相等，*或者*\n",
    "* * 其中一个维度的大小必须为 1，*或者*\n",
    "* * 其中一个张量中不存在该维度\n",
    "\n",
    "形状相同的张量当然是可以“广播”的，如您之前所见。\n",
    "\n",
    "以下是一些符合上述规则并允许广播的示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# 广播示例\n",
    "a =     torch.ones(4, 3, 2)\n",
    "\n",
    "b = a * torch.rand(   3, 2) # (4, 3, 2) * (3, 2)\n",
    "# (3, 2) 的维度从后往前匹配 (3, 2)\n",
    "# (3, 2) 的第一个维度不存在，可以广播\n",
    "print(b.shape)\n",
    "\n",
    "c = a * torch.rand(   3, 1) # (4, 3, 2) * (3, 1)\n",
    "# (3, 1) 的最后一个维度是 1，可以广播\n",
    "# (3, 1) 的倒数第二个维度是 3，与 a 匹配\n",
    "# (3, 1) 的第一个维度不存在，可以广播\n",
    "print(c.shape)\n",
    "\n",
    "d = a * torch.rand(   1, 2) # (4, 3, 2) * (1, 2)\n",
    "# (1, 2) 的最后一个维度是 2，与 a 匹配\n",
    "# (1, 2) 的倒数第二个维度是 1，可以广播\n",
    "# (1, 2) 的第一个维度不存在，可以广播\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仔细查看上面每个张量的值：\n",
    "* 创建 `b` 的乘法运算在 `a` 的每一“层”（第一个维度）上进行了广播。\n",
    "* 对于 `c`，运算在 `a` 的每一层（第一个维度）和每一行（第二个维度）上进行了广播——每个 3 元素列都是相同的。\n",
    "* 对于 `d`，我们将其反转——现在每*行*（第二个维度）都是相同的，跨层（第一个维度）和列（第三个维度）。\n",
    "\n",
    "有关广播的更多信息，请参阅 [PyTorch 文档](https://pytorch.org/docs/stable/notes/broadcasting.html)。\n",
    "\n",
    "以下是一些尝试广播但会失败的示例：\n",
    "\n",
    "**注意：以下代码单元会抛出运行时错误。这是故意的。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 广播失败示例\n",
    "a =     torch.ones(4, 3, 2)\n",
    "\n",
    "# b = a * torch.rand(4, 3)    # 错误：维度必须从后往前匹配。这里 (3) 和 (2) 不匹配\n",
    "\n",
    "# c = a * torch.rand(   2, 3) # 错误：最后一个维度 (3) 和 (2) 不匹配，倒数第二个维度 (2) 和 (3) 也不匹配\n",
    "\n",
    "# d = a * torch.rand((0, ))   # 错误：无法与空张量进行广播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更多张量数学运算\n",
    "\n",
    "PyTorch 张量有超过三百种可以对其执行的操作。\n",
    "\n",
    "以下是一些主要类别操作的小样本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "常见函数:\n",
      "tensor([[0.2424, 0.0866, 0.8702, 0.3355],\n",
      "        [0.5652, 0.7337, 0.9955, 0.0110]])\n",
      "tensor([[-0., -0., -0., 1.],\n",
      "        [1., -0., -0., -0.]])\n",
      "tensor([[-1., -1., -1.,  0.],\n",
      "        [ 0., -1., -1., -1.]])\n",
      "tensor([[-0.2424, -0.0866, -0.5000,  0.3355],\n",
      "        [ 0.5000, -0.5000, -0.5000, -0.0110]])\n",
      "\n",
      "正弦和反正弦:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 0.7854])\n",
      "\n",
      "按位异或:\n",
      "tensor([3, 2, 1], dtype=torch.int32)\n",
      "\n",
      "广播的元素级相等比较:\n",
      "tensor([[ True, False],\n",
      "        [False, False]])\n",
      "\n",
      "归约操作:\n",
      "tensor(4.)\n",
      "4.0\n",
      "tensor(2.5000)\n",
      "tensor(1.2910)\n",
      "tensor(24.)\n",
      "tensor([1, 2])\n",
      "\n",
      "向量和矩阵:\n",
      "tensor([ 0.,  0., -1.])\n",
      "tensor([[0.3857, 0.9883],\n",
      "        [0.4762, 0.7242]])\n",
      "tensor([[1.1572, 2.9650],\n",
      "        [1.4287, 2.1726]])\n",
      "torch.return_types.svd(\n",
      "U=tensor([[-0.7758, -0.6310],\n",
      "        [-0.6310,  0.7758]]),\n",
      "S=tensor([4.0883, 0.4211]),\n",
      "V=tensor([[-0.4401,  0.8980],\n",
      "        [-0.8980, -0.4401]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dj/n08gnc912pl9qnn2yc_jkrg80000gn/T/ipykernel_54430/3814366473.py:46: UserWarning: Using torch.cross without specifying the dim arg is deprecated.\n",
      "Please either pass the dim explicitly or simply use torch.linalg.cross.\n",
      "The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Cross.cpp:66.)\n",
      "  print(torch.cross(v2, v1)) # 计算向量叉积 (v2 x v1)\n"
     ]
    }
   ],
   "source": [
    "# 常见函数\n",
    "a = torch.rand(2, 4) * 2 - 1 # 生成 [-1, 1) 之间的随机数\n",
    "print('常见函数:')\n",
    "print(torch.abs(a))   # 绝对值\n",
    "print(torch.ceil(a))  # 向上取整\n",
    "print(torch.floor(a)) # 向下取整\n",
    "print(torch.clamp(a, -0.5, 0.5)) # 将值限制在 [-0.5, 0.5] 区间内\n",
    "\n",
    "# 三角函数及其反函数\n",
    "angles = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "sines = torch.sin(angles) # 计算正弦值\n",
    "inverses = torch.asin(sines) # 计算反正弦值\n",
    "print('\\n正弦和反正弦:')\n",
    "print(angles)\n",
    "print(sines)\n",
    "print(inverses)\n",
    "\n",
    "# 位运算\n",
    "print('\\n按位异或:')\n",
    "b = torch.tensor([1, 5, 11], dtype=torch.int)\n",
    "c = torch.tensor([2, 7, 10], dtype=torch.int)\n",
    "print(torch.bitwise_xor(b, c)) # 按位异或 (XOR)\n",
    "\n",
    "# 比较:\n",
    "print('\\n广播的元素级相等比较:')\n",
    "d = torch.tensor([[1., 2.], [3., 4.]])\n",
    "e = torch.ones(1, 2)  # 许多比较操作支持广播！\n",
    "print(torch.eq(d, e)) # 比较 d 和 e 是否相等 (按元素，支持广播)\n",
    "\n",
    "# 归约:\n",
    "print('\\n归约操作:')\n",
    "print(torch.max(d))        # 返回张量中的最大值 (单个元素张量)\n",
    "print(torch.max(d).item()) # 使用 .item() 从单元素张量中提取 Python 数字\n",
    "print(torch.mean(d.float())) # 计算平均值 (需要浮点类型)\n",
    "print(torch.std(d.float()))  # 计算标准差 (需要浮点类型)\n",
    "print(torch.prod(d))       # 计算所有元素的乘积\n",
    "print(torch.unique(torch.tensor([1, 2, 1, 2, 1, 2]))) # 找出并返回唯一元素\n",
    "\n",
    "# 向量和线性代数操作\n",
    "v1 = torch.tensor([1., 0., 0.])         # x 单位向量\n",
    "v2 = torch.tensor([0., 1., 0.])         # y 单位向量\n",
    "m1 = torch.rand(2, 2)                   # 随机矩阵\n",
    "m2 = torch.tensor([[3., 0.], [0., 3.]]) # 3 倍单位矩阵\n",
    "\n",
    "print('\\n向量和矩阵:')\n",
    "print(torch.cross(v2, v1)) # 计算向量叉积 (v2 x v1)\n",
    "print(m1)\n",
    "m3 = torch.matmul(m1, m2) # 矩阵乘法\n",
    "print(m3)                  # m3 是 m1 的三倍\n",
    "print(torch.svd(m3))       # 奇异值分解 (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是一个小样本。有关更多详细信息和数学函数的完整清单，请查看 [文档](https://pytorch.org/docs/stable/torch.html#math-operations)。\n",
    "\n",
    "### 就地修改张量\n",
    "\n",
    "对张量的大多数二元运算将返回第三个新张量。当我们说 `c = a * b`（其中 `a` 和 `b` 是张量）时，新张量 `c` 将占据与其他张量不同的内存区域。\n",
    "\n",
    "不过，有时您可能希望就地修改张量——例如，如果您正在进行一个可以丢弃中间值的元素级计算。为此，大多数数学函数都有一个带有下划线（`_`）的版本，可以就地修改张量。\n",
    "\n",
    "例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "\n",
      "b:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n"
     ]
    }
   ],
   "source": [
    "# 就地修改张量示例\n",
    "a = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "print('a:')\n",
    "print(a)\n",
    "print(torch.sin(a))   # 此操作在内存中创建一个新张量\n",
    "print(a)              # a 没有改变\n",
    "\n",
    "b = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "print('\\nb:')\n",
    "print(b)\n",
    "print(torch.sin_(b))  # 注意下划线 _ 表示就地操作\n",
    "print(b)              # b 已被就地修改"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于算术运算，有一些行为类似的函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "之前:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0.0776, 0.4004],\n",
      "        [0.9877, 0.0352]])\n",
      "\n",
      "加法后 (就地):\n",
      "tensor([[1.0776, 1.4004],\n",
      "        [1.9877, 1.0352]])\n",
      "tensor([[1.0776, 1.4004],\n",
      "        [1.9877, 1.0352]])\n",
      "tensor([[0.0776, 0.4004],\n",
      "        [0.9877, 0.0352]])\n",
      "\n",
      "乘法后 (就地)\n",
      "tensor([[0.0060, 0.1603],\n",
      "        [0.9756, 0.0012]])\n",
      "tensor([[0.0060, 0.1603],\n",
      "        [0.9756, 0.0012]])\n"
     ]
    }
   ],
   "source": [
    "# 就地算术运算示例\n",
    "a = torch.ones(2, 2)\n",
    "b = torch.rand(2, 2)\n",
    "\n",
    "print('之前:')\n",
    "print(a)\n",
    "print(b)\n",
    "print('\\n加法后 (就地):')\n",
    "print(a.add_(b)) # a 被就地修改 (a = a + b)\n",
    "print(a)         # a 的值已改变\n",
    "print(b)         # b 的值未改变\n",
    "print('\\n乘法后 (就地)')\n",
    "print(b.mul_(b)) # b 被就地修改 (b = b * b)\n",
    "print(b)         # b 的值已改变"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意这些就地算术函数是 `torch.Tensor` 对象上的方法，而不是像许多其他函数（例如 `torch.sin()`）那样附加到 `torch` 模块上。如您从 `a.add_(b)` 中看到的，*调用张量是那个就地改变的张量。*\n",
    "\n",
    "还有另一种选择可以将计算结果放入现有的分配张量中。我们迄今为止看到的许多方法和函数——包括创建方法！——都有一个 `out` 参数，可以让您指定一个张量来接收输出。如果 `out` 张量具有正确的形状和 `dtype`，这可以在不进行新的内存分配的情况下发生："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[0.4101, 0.1728],\n",
      "        [0.8007, 0.7183]])\n",
      "tensor([[0.8441, 0.9004],\n",
      "        [0.3995, 0.6324]])\n"
     ]
    }
   ],
   "source": [
    "# 使用 out 参数示例\n",
    "a = torch.rand(2, 2)\n",
    "b = torch.rand(2, 2)\n",
    "c = torch.zeros(2, 2) # 预先分配用于存储结果的张量\n",
    "old_id = id(c)        # 记录 c 的内存地址\n",
    "\n",
    "print(c)\n",
    "d = torch.matmul(a, b, out=c) # 将 a 和 b 的矩阵乘法结果存入 c\n",
    "print(c)                # c 的内容已改变\n",
    "\n",
    "assert c is d           # 测试 c 和 d 是否指向同一个对象 (是的)\n",
    "assert id(c) == old_id  # 确保 c 仍然是原来的内存对象 (是的)\n",
    "\n",
    "torch.rand(2, 2, out=c) # 创建方法也可以使用 out 参数！将新的随机值存入 c\n",
    "print(c)                # c 再次改变\n",
    "assert id(c) == old_id  # 仍然是同一个内存对象！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 复制张量\n",
    "\n",
    "与 Python 中的任何对象一样，将张量分配给变量会使变量成为张量的*标签*，而不会复制它。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1., 561.],\n",
      "        [  1.,   1.]])\n"
     ]
    }
   ],
   "source": [
    "# 张量标签示例 (赋值是引用)\n",
    "a = torch.ones(2, 2)\n",
    "b = a # b 现在是 a 的一个标签，指向同一个张量对象\n",
    "\n",
    "a[0][1] = 561  # 我们改变了 a...\n",
    "print(b)       # ...b 也被改变了，因为它们指向同一个内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是如果您想要一个单独的副本来处理数据怎么办？`clone()` 方法可以满足您的需求："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True],\n",
      "        [True, True]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 使用 clone() 方法复制张量\n",
    "a = torch.ones(2, 2)\n",
    "b = a.clone() # 创建 a 的一个独立副本\n",
    "\n",
    "assert b is not a      # b 和 a 是内存中的不同对象...\n",
    "print(torch.eq(a, b))  # ...但它们仍然具有相同的内容！\n",
    "\n",
    "a[0][1] = 561          # a 改变了...\n",
    "print(b)               # ...但 b 仍然是全 1，因为它是一个独立的副本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**使用 `clone()` 时需要注意一个重要事项。** 如果您的源张量启用了自动梯度，那么克隆也会启用自动梯度。**这将在自动梯度的视频中更深入地介绍，**但如果您想要简化版的细节，请继续阅读。\n",
    "\n",
    "*在许多情况下，这将是您想要的结果。* 例如，如果您的模型在其 `forward()` 方法中有多个计算路径，并且*原始张量及其克隆都对模型的输出有贡献*，那么为了实现模型学习，您希望两个张量都启用自动梯度。如果您的源张量启用了自动梯度（如果它是学习权重的一组或从涉及权重的计算中得出的结果，它通常会启用自动梯度），那么您将获得您想要的结果。\n",
    "\n",
    "另一方面，如果您正在进行一个计算，其中*原始张量和其克隆都不需要跟踪梯度*，那么只要源张量关闭了自动梯度，您就可以继续。\n",
    "\n",
    "*还有第三种情况，* 想象一下您正在模型的 `forward()` 函数中进行计算，其中默认情况下所有内容都启用了梯度，但您希望在中途提取一些值以生成一些指标。在这种情况下，您*不希望*源张量的克隆副本跟踪梯度——关闭自动梯度的历史记录跟踪可以提高性能。为此，您可以在源张量上使用 `.detach()` 方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9464, 0.0113],\n",
      "        [0.5183, 0.9807]], requires_grad=True)\n",
      "tensor([[0.9464, 0.0113],\n",
      "        [0.5183, 0.9807]], grad_fn=<CloneBackward0>)\n",
      "tensor([[0.9464, 0.0113],\n",
      "        [0.5183, 0.9807]])\n",
      "tensor([[0.9464, 0.0113],\n",
      "        [0.5183, 0.9807]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 使用 detach() 方法分离张量 (与梯度相关)\n",
    "a = torch.rand(2, 2, requires_grad=True) # 创建张量并启用梯度跟踪\n",
    "print(a)\n",
    "\n",
    "b = a.clone() # 克隆张量，梯度跟踪也被克隆\n",
    "print(b) # b 也有 requires_grad=True，并且有梯度函数 (grad_fn)\n",
    "\n",
    "c = a.detach().clone() # 先分离计算图，再克隆\n",
    "print(c) # c 没有 requires_grad=True，也没有 grad_fn\n",
    "\n",
    "print(a) # a 本身没有改变，仍然 requires_grad=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里发生了什么？\n",
    "\n",
    "* 我们创建了 `a`，并启用了 `requires_grad=True`。**我们尚未介绍此可选参数，但将在自动梯度单元中介绍。**\n",
    "* 当我们打印 `a` 时，它告诉我们属性 `requires_grad=True`——这意味着启用了自动梯度和计算历史记录跟踪。\n",
    "* 我们克隆了 `a` 并将其标记为 `b`。当我们打印 `b` 时，我们可以看到它正在跟踪其计算历史——它继承了 `a` 的自动梯度设置，并添加到计算历史中。\n",
    "* 我们将 `a` 克隆到 `c`，但我们首先调用了 `detach()`。\n",
    "* 打印 `c`，我们看到没有计算历史，也没有 `requires_grad=True`。\n",
    "\n",
    "`detach()` 方法*将张量从其计算历史中分离出来。* 它表示，“接下来要做的事情就像关闭了自动梯度一样。” 它*不会*改变 `a`——您可以看到我们在最后再次打印 `a` 时，它保留了其 `requires_grad=True` 属性。\n",
    "\n",
    "## 移动到 GPU\n",
    "\n",
    "PyTorch 的一个主要优势是其在支持 CUDA 的 Nvidia GPU 上的强大加速。（“CUDA” 代表*统一计算设备架构*，这是 Nvidia 的并行计算平台。）到目前为止，我们所做的一切都在 CPU 上。我们如何移动到更快的硬件？\n",
    "\n",
    "首先，我们应该使用 `is_available()` 方法检查是否有 GPU 可用。\n",
    "\n",
    "**注意：如果您没有支持 CUDA 的 GPU 和安装了 CUDA 驱动程序，本节中的可执行代码单元将不会执行任何与 GPU 相关的代码。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抱歉，只有 CPU 可用。\n"
     ]
    }
   ],
   "source": [
    "# 检查 CUDA (GPU 支持) 是否可用\n",
    "if torch.cuda.is_available():\n",
    "    print('我们有一个可用的 GPU!')\n",
    "else:\n",
    "    print('抱歉，只有 CPU 可用。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一旦我们确定有一个或多个 GPU 可用，我们需要将数据放在 GPU 可以访问的地方。您的 CPU 在计算机的 RAM 中对数据进行计算。您的 GPU 有其专用的内存。每当您想在某个设备上执行计算时，您必须将该计算所需的所有数据移动到该设备可访问的内存中。（通俗地说，“将数据移动到 GPU 可访问的内存”被简称为“将数据移动到 GPU”。）\n",
    "\n",
    "有多种方法可以将数据放到目标设备上。您可以在创建时执行此操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抱歉，只有 CPU 可用。\n"
     ]
    }
   ],
   "source": [
    "# 在创建张量时指定设备为 GPU\n",
    "if torch.cuda.is_available():\n",
    "    gpu_rand = torch.rand(2, 2, device='cuda') # 使用 device 参数\n",
    "    print(gpu_rand)\n",
    "else:\n",
    "    print('抱歉，只有 CPU 可用。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认情况下，新张量是在 CPU 上创建的，因此当我们想在 GPU 上创建张量时，必须使用可选的 `device` 参数来指定。您可以看到，当我们打印新张量时，PyTorch 会告知我们它所在的设备（如果不在 CPU 上）。\n",
    "\n",
    "您可以使用 `torch.cuda.device_count()` 查询 GPU 的数量。如果您有多个 GPU，可以通过索引指定它们：`device='cuda:0'`、`device='cuda:1'` 等。\n",
    "\n",
    "作为一种编码实践，到处使用字符串常量来指定我们的设备是相当脆弱的。在理想情况下，无论您是在 CPU 还是 GPU 硬件上，您的代码都应该能够稳健地执行。您可以通过创建一个设备句柄来实现这一点，该句柄可以传递给您的张量，而不是字符串："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用的设备: cpu\n",
      "tensor([[0.6545, 0.4144],\n",
      "        [0.0696, 0.4648]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个设备句柄，以便代码在 CPU 和 GPU 上都能运行\n",
    "if torch.cuda.is_available():\n",
    "    my_device = torch.device('cuda') # 获取 GPU 设备\n",
    "else:\n",
    "    my_device = torch.device('cpu')  # 获取 CPU 设备\n",
    "print('使用的设备: {}'.format(my_device))\n",
    "\n",
    "# 使用设备句柄创建张量\n",
    "x = torch.rand(2, 2, device=my_device)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果您有一个已存在于某个设备上的张量，您可以使用 `to()` 方法将其移动到另一个设备。以下代码行在 CPU 上创建一个张量，并将其移动到您在先前单元格中获取的设备句柄所指向的设备。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y 初始设备: cpu\n",
      "y 移动后设备: cpu\n"
     ]
    }
   ],
   "source": [
    "# 将已存在的张量移动到指定设备\n",
    "y = torch.rand(2, 2) # 默认在 CPU 上创建\n",
    "print(f'y 初始设备: {y.device}')\n",
    "y = y.to(my_device) # 使用 .to() 方法移动到 my_device (CPU 或 GPU)\n",
    "print(f'y 移动后设备: {y.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重要的是要知道，为了进行涉及两个或多个张量的计算，*所有张量必须位于同一设备上*。无论您是否有可用的 GPU 设备，以下代码都将引发运行时错误：\n",
    "\n",
    "```python\n",
    "# 错误示例：不同设备上的张量无法直接运算\n",
    "x = torch.rand(2, 2) # 在 CPU 上\n",
    "y = torch.rand(2, 2, device='cuda') # 假设在 GPU 上\n",
    "# z = x + y  # 这行会抛出异常，因为 x 和 y 在不同设备上\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 操作张量形状\n",
    "\n",
    "有时，您需要更改张量的形状。下面，我们将看一些常见的情况以及如何处理它们。\n",
    "\n",
    "### 更改维度数量\n",
    "\n",
    "您可能需要更改维度数量的一种情况是将单个输入实例传递给模型。PyTorch 模型通常期望*批量*输入。\n",
    "\n",
    "例如，假设有一个处理 3 x 226 x 226 图像的模型——一个具有 3 个颜色通道的 226 像素正方形。当您加载并转换它时，您将得到一个形状为 `(3, 226, 226)` 的张量。但是，您的模型期望输入的形状为 `(N, 3, 226, 226)`，其中 `N` 是批次中的图像数量。那么如何制作一个包含单个图像的批次呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 的形状: torch.Size([3, 226, 226])\n",
      "b 的形状: torch.Size([1, 3, 226, 226])\n"
     ]
    }
   ],
   "source": [
    "# 使用 unsqueeze 添加批次维度\n",
    "a = torch.rand(3, 226, 226) # 单个图像 (通道, 高, 宽)\n",
    "b = a.unsqueeze(0) # 在第 0 维添加一个维度 (批次大小=1)\n",
    "\n",
    "print(f'a 的形状: {a.shape}')\n",
    "print(f'b 的形状: {b.shape}') # (1, 3, 226, 226)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`unsqueeze()` 方法添加一个大小为 1 的维度。`unsqueeze(0)` 将其添加为新的第零维度——现在您有了一个大小为 1 的批次！\n",
    "\n",
    "那么，如果那是 *un*squeeze（解压缩/增加维度）？我们所说的 squeeze（压缩/减少维度）是什么意思？我们利用了这样一个事实：任何大小为 1 的维度*不会*改变张量中的元素数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[0.5057]]]]])\n"
     ]
    }
   ],
   "source": [
    "# 包含多个大小为 1 的维度的张量\n",
    "c = torch.rand(1, 1, 1, 1, 1)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "继续上面的例子，假设模型的输出是每个输入的 20 元素向量。那么您会期望输出的形状为 `(N, 20)`，其中 `N` 是输入批次中的实例数。这意味着对于我们的单输入批次，我们将得到形状为 `(1, 20)` 的输出。\n",
    "\n",
    "如果您想对该输出进行一些*非批量*计算——比如只需要一个 20 元素的向量，该怎么办？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 的形状: torch.Size([1, 20])\n",
      "tensor([[0.9335, 0.9769, 0.2792, 0.3277, 0.5210, 0.7349, 0.7823, 0.8637, 0.1891,\n",
      "         0.3952, 0.9176, 0.8960, 0.4887, 0.8625, 0.6191, 0.9935, 0.1844, 0.6138,\n",
      "         0.6854, 0.0438]])\n",
      "b 的形状: torch.Size([20])\n",
      "tensor([0.9335, 0.9769, 0.2792, 0.3277, 0.5210, 0.7349, 0.7823, 0.8637, 0.1891,\n",
      "        0.3952, 0.9176, 0.8960, 0.4887, 0.8625, 0.6191, 0.9935, 0.1844, 0.6138,\n",
      "        0.6854, 0.0438])\n",
      "c 的形状: torch.Size([2, 2])\n",
      "d 的形状: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# 使用 squeeze 移除大小为 1 的维度\n",
    "a = torch.rand(1, 20) # 模拟批次大小为 1 的输出\n",
    "print(f'a 的形状: {a.shape}')\n",
    "print(a)\n",
    "\n",
    "b = a.squeeze(0) # 移除第 0 维 (批次维度)\n",
    "print(f'b 的形状: {b.shape}') # (20,)\n",
    "print(b)\n",
    "\n",
    "c = torch.rand(2, 2)\n",
    "print(f'c 的形状: {c.shape}')\n",
    "\n",
    "d = c.squeeze(0) # 尝试移除大小不为 1 的维度 (第 0 维大小为 2)\n",
    "print(f'd 的形状: {d.shape}') # 形状不变，因为第 0 维大小不是 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从形状可以看出，我们的二维张量现在是一维的，如果您仔细观察上面单元格的输出，您会看到打印 `a` 时显示了一组“额外”的方括号 `[]`，因为它有一个额外的维度。\n",
    "\n",
    "您只能 `squeeze()` 大小为 1 的维度。请看上面我们尝试在 `c` 中压缩大小为 2 的维度，结果返回了与开始时相同的形状。调用 `squeeze()` 和 `unsqueeze()` 只能作用于大小为 1 的维度，因为否则会改变张量中的元素数量。\n",
    "\n",
    "您可能使用 `unsqueeze()` 的另一个地方是简化广播。回想一下上面我们有以下代码的例子：\n",
    "\n",
    "```python\n",
    "a =     torch.ones(4, 3, 2)\n",
    "\n",
    "c = a * torch.rand(   3, 1) # 第 3 维 = 1，第 2 维与 a 相同\n",
    "print(c)\n",
    "```\n",
    "\n",
    "这样做的最终效果是将操作广播到维度 0 和 2 上，导致随机的 3 x 1 张量与 `a` 中的每个 3 元素列进行逐元素相乘。\n",
    "\n",
    "如果随机向量只是一个 3 元素的向量会怎样？我们将失去进行广播的能力，因为根据广播规则，最后的维度将不匹配。`unsqueeze()` 可以解决这个问题："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c 的形状: torch.Size([3, 1])\n",
      "torch.Size([4, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# 使用 unsqueeze 辅助广播\n",
    "a = torch.ones(4, 3, 2)\n",
    "b = torch.rand(   3)     # b 的形状是 (3,)\n",
    "# 尝试 a * b 会报错，因为 (4, 3, 2) 和 (3,) 无法广播 (最后维度 2 != 3)\n",
    "\n",
    "c = b.unsqueeze(1)       # 将 b 变形为 (3, 1)\n",
    "print(f'c 的形状: {c.shape}')\n",
    "# 现在可以广播了: (4, 3, 2) * (3, 1)\n",
    "# (3, 1) 的最后维度是 1，可以广播\n",
    "# (3, 1) 的倒数第二维度是 3，与 a 匹配\n",
    "# (3, 1) 的第一个维度不存在，可以广播\n",
    "print((a * c).shape)     # 广播后的形状是 (4, 3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`squeeze()` 和 `unsqueeze()` 方法也有就地版本，`squeeze_()` 和 `unsqueeze_()`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始形状: torch.Size([3, 226, 226])\n",
      "添加维度后形状: torch.Size([1, 3, 226, 226])\n"
     ]
    }
   ],
   "source": [
    "# 就地 unsqueeze\n",
    "batch_me = torch.rand(3, 226, 226)\n",
    "print(f'原始形状: {batch_me.shape}')\n",
    "batch_me.unsqueeze_(0) # 就地在第 0 维添加维度\n",
    "print(f'添加维度后形状: {batch_me.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有时您会想更彻底地改变张量的形状，同时仍然保留元素的数量及其内容。这种情况发生的一个例子是在模型的卷积层和线性层之间的接口处——这在图像分类模型中很常见。卷积核将产生形状为 *特征数 x 宽度 x 高度* 的输出张量，但随后的线性层期望一维输入。`reshape()` 可以为您做到这一点，前提是您请求的维度产生的元素数量与输入张量相同："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始形状: torch.Size([6, 20, 20])\n",
      "展平后形状: torch.Size([2400])\n",
      "使用 torch.reshape 展平后形状: torch.Size([2400])\n"
     ]
    }
   ],
   "source": [
    "# 使用 reshape 改变张量形状 (展平)\n",
    "output3d = torch.rand(6, 20, 20) # 模拟卷积层输出 (批次, 特征, 尺寸)\n",
    "print(f'原始形状: {output3d.shape}')\n",
    "\n",
    "input1d = output3d.reshape(6 * 20 * 20) # 将其展平成一维向量\n",
    "print(f'展平后形状: {input1d.shape}')\n",
    "\n",
    "# 也可以作为 torch 模块的函数调用:\n",
    "print(f'使用 torch.reshape 展平后形状: {torch.reshape(output3d, (6 * 20 * 20,)).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(注意：上面单元格最后一行中的 `(6 * 20 * 20,)` 参数是因为 PyTorch 在指定张量形状时期望一个 **元组**——但是当形状是方法的第一个参数时，它允许我们作弊，只使用一系列整数。在这里，我们必须添加括号和逗号来说服该方法这确实是一个单元素元组。)*\n",
    "\n",
    "如果可能，`reshape()` 将返回要更改的张量的*视图*——也就是说，一个单独的张量对象，查看的是相同的底层内存区域。*这很重要：* 这意味着对源张量所做的任何更改都将反映在该张量的视图中，除非您 `clone()` 它。\n",
    "\n",
    "在某些条件下（超出了本介绍的范围），`reshape()` 必须返回一个携带数据副本的张量。有关更多信息，请参阅[文档](https://pytorch.org/docs/stable/torch.html#torch.reshape)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy 桥接\n",
    "\n",
    "在上面关于广播的部分中，提到 PyTorch 的广播语义与 NumPy 兼容——但 PyTorch 和 NumPy 之间的关系甚至比这更深。\n",
    "\n",
    "如果您现有的机器学习或科学代码中数据存储在 NumPy ndarray 中，您可能希望将相同的数据表示为 PyTorch 张量，无论是为了利用 PyTorch 的 GPU 加速，还是其用于构建机器学习模型的高效抽象。在 ndarray 和 PyTorch 张量之间切换很容易："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# NumPy 与 PyTorch 张量之间的转换\n",
    "import numpy as np\n",
    "\n",
    "# 从 NumPy 数组创建 PyTorch 张量\n",
    "numpy_array = np.ones((2, 3)) # 创建一个 NumPy 数组\n",
    "print(numpy_array)\n",
    "\n",
    "pytorch_tensor = torch.from_numpy(numpy_array) # 从 NumPy 数组创建张量\n",
    "print(pytorch_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 创建了一个与 NumPy 数组形状相同且包含相同数据的张量，甚至保留了 NumPy 默认的 64 位浮点数据类型。\n",
    "\n",
    "转换也可以同样容易地反向进行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0171, 0.2775, 0.0935],\n",
      "        [0.1874, 0.3136, 0.7793]])\n",
      "[[0.01713592 0.27750254 0.09351915]\n",
      " [0.18740034 0.31359702 0.77929854]]\n"
     ]
    }
   ],
   "source": [
    "# 从 PyTorch 张量创建 NumPy 数组\n",
    "pytorch_rand = torch.rand(2, 3)\n",
    "print(pytorch_rand)\n",
    "\n",
    "numpy_rand = pytorch_rand.numpy() # 将张量转换为 NumPy 数组\n",
    "print(numpy_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重要的是要知道，这些转换后的对象与其源对象使用的是*相同的底层内存*，这意味着对一个对象的更改会反映在另一个对象上："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.],\n",
      "        [ 1., 23.,  1.]], dtype=torch.float64)\n",
      "[[ 0.01713592  0.27750254  0.09351915]\n",
      " [ 0.18740034 17.          0.77929854]]\n"
     ]
    }
   ],
   "source": [
    "# NumPy 和 PyTorch 共享内存\n",
    "numpy_array[1, 1] = 23 # 修改 NumPy 数组\n",
    "print(pytorch_tensor) # PyTorch 张量也改变了\n",
    "\n",
    "pytorch_rand[1, 1] = 17 # 修改 PyTorch 张量\n",
    "print(numpy_rand)     # NumPy 数组也改变了"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
